import torch
import torch.optim as optim
import random
from collections import deque
from environment import AndroidEnv
from gnn import GNN, create_graph_from_gui
from dqn import DQN
from mctspy.tree import MCTS

def main():
    # Khởi tạo môi trường
    apk_path = "C:\\APKs\\MyExpenses-r554-debug.apk"  # Thay bằng đường dẫn đến APK của bạn
    env = AndroidEnv(apk_path)

    # Khởi tạo mô hình GNN và DQN
    gnn_model = GNN()
    dqn_model = DQN()
    optimizer = optim.Adam(dqn_model.parameters(), lr=0.001)

    # Thiết lập tham số RL
    memory = deque(maxlen=10000)
    gamma = 0.95
    epsilon = 1.0
    epsilon_min = 0.01
    epsilon_decay = 0.995
    batch_size = 32

    # Huấn luyện
    for episode in range(100):
        state = env.reset()
        state = torch.tensor(state, dtype=torch.float).permute(2, 0, 1).unsqueeze(0)
        total_reward = 0

        for t in range(1000):
            # Tích hợp MCTS (ARES)
            if random.random() < 0.1:  # Dùng MCTS 10% thời gian
                mcts = MCTS()
                action = mcts.search(initial_state=state, n_simulations=50)
            else:
                # Dùng DQN với epsilon-greedy
                if random.random() <= epsilon:
                    action = env.action_space.sample()
                else:
                    with torch.no_grad():
                        q_values = dqn_model(state)
                        action = q_values.argmax().item()

            # Thực hiện hành động
            next_state, reward, done, _ = env.step(action)
            next_state = torch.tensor(next_state, dtype=torch.float).permute(2, 0, 1).unsqueeze(0)
            memory.append((state, action, reward, next_state, done))
            state = next_state
            total_reward += reward

            # Huấn luyện DQN
            if len(memory) > batch_size:
                batch = random.sample(memory, batch_size)
                states, actions, rewards, next_states, dones = zip(*batch)
                states = torch.cat(states)
                actions = torch.tensor(actions)
                rewards = torch.tensor(rewards)
                next_states = torch.cat(next_states)
                dones = torch.tensor(dones)

                q_values = dqn_model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
                next_q_values = dqn_model(next_states).max(1)[0]
                targets = rewards + (1 - dones) * gamma * next_q_values
                loss = F.mse_loss(q_values, targets)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            if done:
                break

        if epsilon > epsilon_min:
            epsilon *= epsilon_decay
        print(f"Episode {episode}, Total Reward: {total_reward}")

    env.close()

if __name__ == "__main__":
    main()